{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Deep Learning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "Tensor = List\n",
    "def shape(tensor:Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor,list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "assert shape([1,2,3]) == [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor:Tensor) -> bool:\n",
    "    return not isinstance(tensor[0],list)\n",
    "\n",
    "assert is_1d([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor:Tensor) -> float:\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)\n",
    "                  for tensor_i in tensor)\n",
    "\n",
    "\n",
    "assert tensor_sum([1,2,3]) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f:Callable[[float],float],tensor:Tensor) -> Tensor:\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f,tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1,[1,2,3]) == [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor:Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0,tensor)\n",
    "\n",
    "assert zeros_like([1,2,3]) == [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f:Callable[[float,],float],\n",
    "                  t1:Tensor,\n",
    "                  t2:Tensor) -> Tensor:\n",
    "    if is_1d(t1):\n",
    "        return [f(x,y) for x,y in zip(t1,t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f,t1_i,t2_i)\n",
    "               for t1_i,t2_i in zip(t1,t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add,[1,2,3],[4,5,6]) == [5,7,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable,Tuple\n",
    "class Layer:\n",
    "    \n",
    "    def forward(self,input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self,gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self,input:Tensor) -> Tensor:\n",
    "        self.sigmoid = tensor_apply(sigmoid,input)\n",
    "        return self.sigmoids\n",
    "    \n",
    "    def backward(self,gradient:Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig,grad:sig*(1-sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from scratch.probability import inverse_normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_uniform(*dims:int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims:int,\n",
    "                 mean: float = 0.0,\n",
    "                 varience: float=1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean+varience*inverse_normal_cdf(random.random())\n",
    "               for _ in range(dims[0])]\n",
    "    \n",
    "assert shape(random_uniform(2,3,4)) == [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims:int,init:str='normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        varience = len(dims) / sum(dims)\n",
    "        return random_normal(*dims,varience=varience)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init:{init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self,\n",
    "                input_dim:int,\n",
    "                output_dim:int,\n",
    "                init: str='xavier')-> None:\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.w = random_tensor(output_dim,input_dim,init=init)\n",
    "        self.b = random_tensor(output_dim,init=init)\n",
    "        \n",
    "    def forward(self,input:Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return  [dot(input, self.w[o] + f.b[o])\n",
    "                for o in range(self.output_dim)]\n",
    "    \n",
    "    def backward(self,gradient:Tensor) -> Tensor:\n",
    "        self.b_grad = gradient\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                       for i in range(self.input_dim)]\n",
    "                      for o in range(self.output_dim)]\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim)) for i in range(self.input_dim)]\n",
    "  \n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w,self.b]\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad,self.b_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    def __init__(self,layers:List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    def backward(self,gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return (grad  for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_net = Sequential([\n",
    "    Linear(input_dim=2,output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2,output_dim=2),\n",
    "    Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self,predicted:Tensor,actual:Tensor) -> float:\n",
    "        raise NotImplementedError\n",
    "    def gradient(self,predicted:Tensor,actual:Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(Loss):\n",
    "    def loss(self,predicted:Tensor,actual:Tensor) -> float:\n",
    "        squared_errors = tensor_combine(\n",
    "        lambda predicted,actual:(predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual)\n",
    "        return tensor_sum(squared_errors)\n",
    "    \n",
    "    def gradient(self,predicted:Tensor,actual:Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "        lambda predicted,actual:2*(predicted - actual),\n",
    "        predicted,\n",
    "        actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradient_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f746058f502c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gradient_step' is not defined"
     ]
    }
   ],
   "source": [
    "theta = gradient_step(theta,grad,-learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b2ecd5e87068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m->\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    def step(self,layer:Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self,learning_rate:float=0.1)->None:\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def step(self,layer:layer) -> None:\n",
    "        for param, grad in zip(layer.params(),layer.grads()):\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param,grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
