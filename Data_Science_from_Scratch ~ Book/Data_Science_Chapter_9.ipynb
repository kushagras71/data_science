{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Getting Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to read lines of text and spits backout the ones \n",
    "# that match a regular experssion\n",
    "import sys, re\n",
    "regex = sys.argv[1]\n",
    "for line in sys.stdin:\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout.wrtie(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Working with some sample text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In literary theory, a text is any object that can be \"read\", whether this object is a work of literature, a street sign, an arrangement of buildings on a city block, or styles of clothing. It is a coherent set of signs that transmits some kind of informative message.[1] This set of signs is considered in terms of the informative message's content, rather than in terms of its physical form or the medium in which it is represented.\n",
      "\n",
      "\n",
      "\n",
      "Within the field of literary criticism, \"text\" also refers to the original information content of a particular piece of writing; that is, the \"text\" of a work is that primal symbolic arrangement of letters as originally composed, apart from later alterations, deterioration, commentary, translations, paratext, etc. Therefore, when literary criticism is concerned with the determination of a \"text\", it is concerned with the distinguishing of the original information content from whatever has been added to or subtracted from that content as it appears in a given textual document (that is, a physical representation of text).\n"
     ]
    }
   ],
   "source": [
    "with open(\"sometext.txt\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        \n",
    "# content from  - https://en.wikipedia.org/wiki/Text_(literary_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "starts_with_A = 0\n",
    "with open(\"sometext.txt\") as f:\n",
    "    for line in f:\n",
    "        if re.match(\"^A\",line):\n",
    "            starts_with_A += 1\n",
    "\n",
    "print(starts_with_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "starts_with_I = 0\n",
    "with open(\"sometext.txt\") as f:\n",
    "    for line in f:\n",
    "        if re.match(\"^I\",line):\n",
    "            starts_with_I += 1\n",
    "\n",
    "print(starts_with_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gmail.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_domain(email:str) -> str:\n",
    "    return email.lower().split(\"@\")[-1]\n",
    "email = \"data_Science@gmail.com\"\n",
    "get_domain(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'mail.com': 1, 'gmail.com': 1, '123_mail.com': 1, 'science.com': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "with open('emails.txt','r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if\"@\" in line)\n",
    "    print(domain_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Delimiter Files</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "6/19/2014 AAPL 91.86\n",
      "6/19/2014 MSFT 41.51\n",
      "6/19/2014 FB 64.34\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "    \n",
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        print(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "OrderedDict([('date', '6/20/2014'), ('symbol', 'AAPL'), ('closing_price', '90.91')])\n",
      "6/20/2014 MSFT 41.68\n",
      "OrderedDict([('date', '6/20/2014'), ('symbol', 'MSFT'), ('closing_price', '41.68')])\n",
      "6/20/2014 FB 64.5\n",
      "OrderedDict([('date', '6/20/2014'), ('symbol', 'FB'), ('closing_price', '64.5')])\n"
     ]
    }
   ],
   "source": [
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter=':')\n",
    "    for dict_row in colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "        print(date, symbol, closing_price)\n",
    "        print(dict_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scraping the Web</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://raw.githubusercontent.com/joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"p1\">This is the first paragraph.</p>\n"
     ]
    }
   ],
   "source": [
    "first_paragraph = soup.find('p')\n",
    "print(first_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first paragraph.\n",
      "\n",
      "['This', 'is', 'the', 'first', 'paragraph.']\n"
     ]
    }
   ],
   "source": [
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "print(first_paragraph_text)\n",
    "print()\n",
    "print(first_paragraph_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1\n",
      "\n",
      "p1\n"
     ]
    }
   ],
   "source": [
    "first_paragraph_id = soup.p['id']\n",
    "first_paragraph_id_2 = soup.p.get('id')\n",
    "print(first_paragraph_id)\n",
    "print()\n",
    "print(first_paragraph_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"p1\">This is the first paragraph.</p>, <p class=\"important\">This is the second paragraph.</p>]\n"
     ]
    }
   ],
   "source": [
    "all_paragraphs = soup.find_all('p')\n",
    "print(all_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"p1\">This is the first paragraph.</p>]\n"
     ]
    }
   ],
   "source": [
    "paragraphs_with_id = [p for p in soup('p') if p.get('id')]\n",
    "print(paragraphs_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"important\">This is the second paragraph.</p>]\n",
      "\n",
      "[<p class=\"important\">This is the second paragraph.</p>]\n",
      "\n",
      "[<p class=\"important\">This is the second paragraph.</p>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "important_paragraphs = soup('p', {'class' : 'important'})\n",
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs3 = [p for p in soup('p')\n",
    "if 'important' in p.get('class', [])]\n",
    "print(important_paragraphs)\n",
    "print()\n",
    "print(important_paragraphs2)\n",
    "print()\n",
    "print(important_paragraphs3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_inside_divs = [span \n",
    "                    for div in soup('div')\n",
    "                    for span in div('span')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://www.house.gov/representatives\"\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "all_urls = [a['href']\n",
    "    for a in soup('a')\n",
    "    if a.has_attr('href')]\n",
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "regex = r\"https?://.*\\.house\\.gov/?$\"\n",
    "good_urls = [url for url in all_urls if re.match(regex,url)]\n",
    "print(len(good_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "good_urls = list(set(good_urls))\n",
    "print(len(good_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
